OpenMM: An Open-source Multimodal Feature Extraction Tool
=============

OpenMM is an open-source tool that can perform multimodal feature extraction. In other words, this tool will allow you to easily extract video, audio, and linguistic features all at once. This tool is intended for human-computer interaction, natural language processing, and machine learning researchers, affective computing community and people interested in building interactive applications based on facial behavior analysis.  This tool builds upon existing repos for visual feature extraction [OpenFace](https://github.com/TadasBaltrusaitis/OpenFace) and audio feature extraction [Covarep](https://github.com/covarep/covarep). I integrate these existing repos with my code for linguistic feature extraction. OpenMM provides a simple way for researchers to extract multimodal features. The tool only requires a video as input and outputs a csv of multimodal features, audio conversion and speech-to-text are handled internally. My hope is that this will help promote more interest and research in building multimodal systems. 

![alt tag](https://github.com/michellemorales/OpenMM/blob/master/images/PipelineVersion3.jpeg)

## How to Use This Library

## Installation
Please see the Wiki for installation instructions.

## Final Note
This repo represents code from my dissertation work. I did my best to ensure that the code runs out of the box, but there are always issues. So please understand that this is research code and not a commercial level product. However, if you encounter any problems/bugs/issues please contact me on github or email me at mmorales@gradcenter.cuny.edu for any bug reports/questions/suggestions.
